{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu, not recommend\n",
      "cpu\n",
      "(67349, 2)\n",
      "(872, 2)\n",
      "(1821, 2)\n",
      "the size of train: 67349, dev:872, test:1821\n",
      "['contains', 'no', 'wit', ',', 'only', 'labored', 'gags'] 0\n",
      "['unflinchingly', 'bleak', 'and', 'desperate'] 0\n",
      "['this', 'film', \"'s\", 'relationship', 'to', 'actual', 'tension', 'is', 'the', 'same', 'as', 'what', 'christmas', '-', 'tree', 'flocking', 'in', 'a', 'spray', 'can', 'is', 'to', 'actual', 'snow', ':', 'a', 'poor', '--', 'if', 'durable', '--', 'imitation', '.']\n",
      "16292\n",
      "1821\n",
      "the size of train_iter: 527, dev_iter:7, test_iter:1\n",
      "0 torch.Size([128, 40]) torch.Size([128])\n",
      "the size of train: 67349, dev:872, test:1821\n",
      "the result of dataset:  ['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units'] 0\n",
      "the size of train_iter: 527, dev_iter:7, test_iter:1\n",
      "the shape of train_x: torch.Size([128, 40]), train_y:torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 导入自己的库\n",
    "from Util.utils import seed_everything,get_device\n",
    "from Util.SST2_data import load_sst2\n",
    "from ModelHandler import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu, not recommend\n",
      "cpu 0\n"
     ]
    }
   ],
   "source": [
    "device, n_gpu=get_device()\n",
    "print(device, n_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载 SST2 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SST2 数据准备\n",
    "\n",
    "text_field = data.Field(tokenize='spacy', lower=True, fix_length=40, batch_first=True)\n",
    "label_field = data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67349, 2)\n",
      "(872, 2)\n",
      "(1821, 2)\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"/home/dudaizhong/Downloads/SST-2/\"\n",
    "train_pd = pd.read_csv(BASE_PATH+'train.tsv', sep='\\t')\n",
    "dev_pd = pd.read_csv(BASE_PATH + 'dev.tsv', sep='\\t')\n",
    "test_pd = pd.read_csv(BASE_PATH + 'test.tsv', sep='\\t')\n",
    "\n",
    "print(train_pd.shape)\n",
    "print(dev_pd.shape)\n",
    "print(test_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of train: 67349, dev:872, test:1821\n",
      "the result of dataset:  ['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units'] 0\n",
      "the size of train_iter: 527, dev_iter:7, test_iter:1\n",
      "the shape of train_x: torch.Size([128, 40]), train_y:torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_iter, dev_iter, test_iter = load_sst2(BASE_PATH, text_field, label_field, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.与维度变换相关函数 view()，permute()，size(), torch.squeeze() / torch.unsqueeze()\n",
    "# 2.Embedding层加载预训练模型的方式：1）copy，2）from_pretrained。\n",
    "\n",
    "class Enet(nn.Module):\n",
    "    def __init__(self,pretrained_embeddings):\n",
    "        super(Enet, self).__init__()\n",
    "#         self.embedding = nn.Embedding(len_vocab,100)\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            pretrained_embeddings, freeze=False)\n",
    "        # LSTM 参数以及输入输出说明：\n",
    "        # 结构参数：LSTM(input_size, hidden_size, num_layers)\n",
    "        # input_size:输入的特征数量\n",
    "        # hidden_size:隐藏的特征数量\n",
    "        # num_layers:层数\n",
    "        self.lstm = nn.LSTM(100,64,3,batch_first=True)#,bidirectional=True)\n",
    "        self.linear = nn.Linear(64,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size,seq_num = x.shape\n",
    "#         print(x.shape) #(batch_size 128, sent_len 40)\n",
    "        vec = self.embedding(x)\n",
    "#         print(vec.shape) #(batch_size 128,sent_len 40,emb_dim 100)\n",
    "        out, (hn, cn) = self.lstm(vec)\n",
    "#         print(out.shape) #(batch_size 128,sent_len 40,64)\n",
    "        out = self.linear(out[:,-1,:])\n",
    "#         print(out.shape) #(batch_size 128,2)\n",
    "        out = F.softmax(out,-1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积网络一般情况\n",
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, filter_sizes):\n",
    "        super(Conv1d, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.convs:\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            nn.init.constant_(m.bias.data, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return [F.relu(conv(x)) for conv in self.convs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,embedding_dim, n_filters, filter_sizes, output_dim,\n",
    "                  pretrained_embeddings):\n",
    "        super(TextCNN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(len_vocab,100)\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            pretrained_embeddings, freeze=False)\n",
    "        \n",
    "        self.convs = Conv1d(embedding_dim,n_filters,filter_sizes)\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size,seq_num = x.shape\n",
    "#         print(x.shape) #(batch_size 128, sent_len 40)\n",
    "        vec = self.embedding(x)\n",
    "#         print(vec.shape) #(batch_size 128,sent_len 40,emb_dim 100)\n",
    "        vec = vec.permute(0,2,1)\n",
    "#         print(vec.shape) #(batch_size 128,emb_dim 100,sent_len 40)\n",
    "        \n",
    "        conved = self.convs(vec)\n",
    "#         print([conv.shape for conv in conved]) # ([128, 100, 40-2+1]), torch.Size([128, 100, 37]), torch.Size([128, 100, 36])\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2)\n",
    "                  for conv in conved]\n",
    "#         print([pool.shape for pool in pooled]) # ([128, 100]), torch.Size([128, 100]), torch.Size([128, 100])\n",
    "        \n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "#         print(cat.shape) # [128, 300]\n",
    "        out = self.fc(cat)\n",
    "#         print(out.shape) # [128, 2]\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 139,  562,   16,  ...,    1,    1,    1],\n",
      "        [  31,    6,    2,  ...,    1,    1,    1],\n",
      "        [  13,  358,    6,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [  52, 2509,    3,  ...,    1,    1,    1],\n",
      "        [  10,   35,  665,  ...,    1,    1,    1],\n",
      "        [4908,    3, 5795,  ...,    1,    1,    1]]) torch.Size([128, 40])\n"
     ]
    }
   ],
   "source": [
    "# 模型单独测试\n",
    "pretrained_embeddings = text_field.vocab.vectors\n",
    "textCNN = TextCNN(100,100,[3,4,5],2,pretrained_embeddings)\n",
    "for i in train_iter:\n",
    "    print(i.text, i.text.shape)\n",
    "    textCNN.forward(i.text)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* epoch: 0 *************************\n",
      "train auc: 0.9364563184657851\n",
      "train loss: 0.3402541098829917\n",
      "*****Checking accuracy on validation set*****\n",
      "batchNum: 7\n",
      "val_auc: 0.8895408693254895\n",
      "val_loss: 0.46029689056532724\n",
      "************************* epoch: 1 *************************\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# seed_everything()\n",
    "\n",
    "train_batch_size, val_batch_size = 2**7, 2**7\n",
    "\n",
    "pretrained_embeddings = text_field.vocab.vectors\n",
    "# model = Enet(pretrained_embeddings)\n",
    "model = TextCNN(100,100,[3,4,5],2,pretrained_embeddings)\n",
    "\n",
    "modelHandlerParams = {}\n",
    "modelHandlerParams['epoch_num'] = 1000000\n",
    "modelHandlerParams['train_batch_size'] = train_batch_size\n",
    "modelHandlerParams['val_batch_size'] = val_batch_size\n",
    "modelHandlerParams['device'] = device\n",
    "\n",
    "modelHandlerParams['model'] = model\n",
    "modelHandler = ModelHandler(modelHandlerParams)\n",
    "\n",
    "# 二分类交叉熵\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "# 调参地方，分别调整为0.1,0.01,0.001，最优为0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01,\n",
    "                       weight_decay=0.00001) # lr sets the learning rate of the optimizer\n",
    "\n",
    "modelHandler.fit(train_iter=train_iter, val_iter=dev_iter,loss_fn=loss_fn,optimizer=optimizer,\n",
    "                 early_stopping_rounds=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
